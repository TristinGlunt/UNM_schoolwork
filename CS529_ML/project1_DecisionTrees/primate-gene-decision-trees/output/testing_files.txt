kaggle1 - 91.8%
Our first submission.

kaggle2 - Accuracy 91.386%
Changed a line in ID3 that does a deep copy of features.



kaggle5 - 91.8%
first gni_index submission

kaggle6 - 90.7%
gni_index, 0.99 confidence

kaggle15 - 96.008%
entropy, 0.95 conf,
random forests with 250 trees, 600 +- 400 elements, 20-40 features

kaggle17 - 96.218%
gni, 0.95 conf,
random forests with 300 trees, 600 +- 400 elements, 15-45 features


kaggle18 - 94.2%
gni, 0.95 conf,
rf with 300 trees, 600 +- 400 elements, 22-40 feats


kaggle19 - 94.1%
gni, 0.95 conf,
rf with 300 trees, 600 +- 500 elements, 10-50 feats

kaggle20 - 95.2%
gni, 0.95 conf,
rf with 150 trees, 200 +- 300 elements, 15-45 feats

kaggle21 -
gni, 0.95 conf,
rf with 250 trees, 200 +- 500 elements, 15-45 feats

kaggle22 - 94.3%
gni, 0.90 conf,
rf with 250 trees, 500-800 ele, 10-50feats

kaggle23 - 95.16%
gni, 0.95 conf,
rf with 500 trees, 200-1000 ele, 15-45 feats

kaggle27 - 94.5%
gni, 0.90 conf,
rf with 500 trees, 600-1200 ele, 20-40 feats

kaggle28 - 96.008%
gni, 0.95 conf
rf with 500 trees, 200-800 ele, 20-40 feats


kaggle30 - 93.487%
gni, 0.95 conf,
rf with 600 trees, 200-600 ele, 15-45 feats

kaggle31 - 89%
gni, 0.99 conf,
rf with 600 trees, 200-600 ele, 10-50 feats


So far my analysis for tuning random forests for this problem...
95% CL looks good, 99% conf looks bad.
accuracy doesn't seem to improve past ~200 trees.


kaggle32 - 96.008%
gni, 0.95 conf,
rf with 300 trees, 400-1000 ele, 15-45 feats


It appears I have found some sort of sweet spot for random forests tuning, but I may
have hit a soft limit of our implementation. We'd likely need more features to be
randomized into this set and perhaps more data.
I think I'm giving up on 97-98% for now.



